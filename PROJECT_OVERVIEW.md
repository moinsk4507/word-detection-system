# ğŸ¤Ÿ Sign Language Alphabet Recognition - Project Overview

## ğŸ“– Introduction

This project provides a complete, production-ready solution for real-time American Sign Language (ASL) alphabet recognition using computer vision and deep learning. It's designed to be accessible for beginners while being robust enough for advanced users to extend and customize.

## ğŸ¯ What This Project Does

The system can:
1. **Detect hands** in real-time using your webcam
2. **Extract hand landmarks** (21 points per hand) using MediaPipe
3. **Classify gestures** into alphabets A-Z using a trained neural network
4. **Display predictions** with confidence scores
5. **Spell words** by continuously detecting letters
6. **Save detected words** to text files

## ğŸ“‚ Project Structure

```
sign-language/
â”‚
â”œâ”€â”€ ğŸ“„ Core Application Files
â”‚   â”œâ”€â”€ app.py                      # Streamlit web interface (main UI)
â”‚   â”œâ”€â”€ detect_sign_language.py     # OpenCV desktop application
â”‚   â”œâ”€â”€ collect_data.py             # Interactive data collection tool
â”‚   â””â”€â”€ train_model.py              # Model training pipeline
â”‚
â”œâ”€â”€ ğŸ› ï¸ Utility Scripts
â”‚   â”œâ”€â”€ utils.py                    # Diagnostic and utility functions
â”‚   â”œâ”€â”€ setup.py                    # Automated setup script
â”‚   â”œâ”€â”€ visualize_model.py          # Model analysis and visualization
â”‚   â””â”€â”€ config.py                   # Configuration parameters
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                   # Complete documentation
â”‚   â”œâ”€â”€ QUICKSTART.md               # 5-minute quick start guide
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md         # This file
â”‚   â””â”€â”€ LICENSE                     # MIT License
â”‚
â”œâ”€â”€ ğŸ“¦ Dependencies
â”‚   â””â”€â”€ requirements.txt            # Python package dependencies
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ Data Directories (created on first run)
â”‚   â”œâ”€â”€ data/                       # Training data (JSON files)
â”‚   â”‚   â”œâ”€â”€ A/
â”‚   â”‚   â”œâ”€â”€ B/
â”‚   â”‚   â””â”€â”€ ... (one folder per letter)
â”‚   â”‚
â”‚   â””â”€â”€ models/                     # Trained models and artifacts
â”‚       â”œâ”€â”€ sign_language_model.keras
â”‚       â”œâ”€â”€ preprocessing_params.json
â”‚       â”œâ”€â”€ training_history.png
â”‚       â””â”€â”€ (visualization outputs)
â”‚
â””â”€â”€ ğŸ“ Generated Files (optional)
    â”œâ”€â”€ detected_words.txt          # Saved words from detection
    â”œâ”€â”€ data_summary.json           # Data collection statistics
    â””â”€â”€ predictions.log             # Prediction history (if enabled)
```

## ğŸ”„ Complete Workflow

### Phase 1: Setup (5 minutes)
```bash
python setup.py
```
- Checks Python version
- Creates directories
- Installs dependencies
- Verifies camera access

### Phase 2: Data Collection (15-20 minutes)
```bash
python collect_data.py
```
- Opens webcam
- Shows which letter to sign
- Collects 100 samples per letter automatically
- Saves data as JSON files

**What's collected:**
- 63 features per sample (21 landmarks Ã— 3 coordinates)
- Label (A-Z)
- Timestamp

### Phase 3: Model Training (5-10 minutes)
```bash
python train_model.py
```
- Loads collected data
- Normalizes features
- Trains neural network
- Validates performance
- Saves model

**Model Architecture:**
```
Input (63 features)
    â†“
Dense(256) + BatchNorm + Dropout
    â†“
Dense(128) + BatchNorm + Dropout
    â†“
Dense(64) + BatchNorm + Dropout
    â†“
Output (26 classes) - Softmax
```

### Phase 4: Detection (Real-time)

**Option A - Web Interface:**
```bash
streamlit run app.py
```
Beautiful, user-friendly web interface with:
- Live video feed
- Large prediction display
- Confidence scores
- Top-3 predictions
- Word spelling mode
- Controls via buttons

**Option B - Desktop Application:**
```bash
python detect_sign_language.py
```
Lightweight OpenCV application with:
- Direct camera access
- Keyboard shortcuts
- Lower latency
- Minimal dependencies

## ğŸ§  How It Works

### 1. Hand Detection (MediaPipe)
```
Camera Frame â†’ MediaPipe Hands â†’ 21 Landmarks (x, y, z)
```
MediaPipe detects 21 key points on the hand:
- Wrist (1 point)
- Thumb (4 points)
- Index finger (4 points)
- Middle finger (4 points)
- Ring finger (4 points)
- Pinky (4 points)

### 2. Feature Extraction
```
21 Landmarks Ã— 3 Coordinates = 63 Features
```
Each landmark has:
- x: Horizontal position (0-1, normalized)
- y: Vertical position (0-1, normalized)
- z: Depth relative to wrist (normalized)

### 3. Preprocessing
```
Features â†’ Normalize (mean, std) â†’ Model Input
```
- Subtract training mean
- Divide by training std
- Ensures consistent scale

### 4. Classification
```
Model Input â†’ Neural Network â†’ 26 Probabilities â†’ Prediction
```
- Forward pass through network
- Softmax activation
- Returns probability for each letter

### 5. Post-processing
```
Prediction â†’ Temporal Smoothing â†’ Final Output
```
- Tracks last 5 predictions
- Requires 3+ consistent predictions
- Reduces jitter and false positives

## ğŸ¨ Key Features Explained

### Confidence Thresholding
Only shows predictions above 60% confidence by default. Adjustable in code or Streamlit UI.

### Word Spelling Mode
- Hold a sign for 1.5 seconds
- Letter automatically adds to word
- Prevents duplicate letters
- Can save to file

### Prediction Smoothing
- Maintains history of recent predictions
- Requires consistency before displaying
- Reduces flickering
- More stable output

### Visualization
- Hand landmarks (green circles)
- Hand connections (red lines)
- Bounding box (green rectangle)
- Large letter display
- Confidence percentage

## ğŸ“Š Performance Metrics

### Expected Accuracy
- **Good lighting, clear gestures:** 90-95%
- **Mixed conditions:** 80-85%
- **Poor conditions:** 60-70%

### Factors Affecting Accuracy
âœ… **Positive:**
- Good, even lighting
- Clear background
- Steady hand
- Centered in frame
- Consistent distance

âŒ **Negative:**
- Dim lighting
- Cluttered background
- Hand motion/blur
- Partial visibility
- Extreme angles

### System Requirements
- **Minimum:** 2 CPU cores, 4GB RAM, integrated camera
- **Recommended:** 4 CPU cores, 8GB RAM, HD webcam
- **Training:** CPU sufficient, GPU optional for faster training

## ğŸ”§ Customization Guide

### Easy Customizations (config.py)
```python
# Change these without touching main code
CONFIDENCE_THRESHOLD = 0.6      # Lower = more sensitive
LETTER_HOLD_TIME = 1.5          # Seconds for word mode
SAMPLES_PER_ALPHABET = 100      # Training samples
```

### Moderate Customizations
```python
# In train_model.py
LAYER_SIZES = [256, 128, 64]    # Larger = more capacity
DROPOUT_RATES = [0.3, 0.3, 0.2] # Higher = more regularization
LEARNING_RATE = 0.001            # Lower = slower, stable
```

### Advanced Customizations
- Add data augmentation
- Implement transfer learning
- Support two hands
- Add motion detection (for J, Z)
- Integrate language model for word prediction

## ğŸ› Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| Low accuracy | Collect more diverse data, improve lighting |
| Camera not opening | Close other apps, check permissions |
| Slow performance | Reduce resolution, optimize model |
| Flickering predictions | Increase smoothing window |
| Model not found | Run training script first |

## ğŸ“ˆ Future Roadmap

### Short-term
- [ ] Add data augmentation
- [ ] Implement word prediction
- [ ] Support for phrases
- [ ] Mobile app version

### Medium-term
- [ ] Multi-hand support
- [ ] Motion-based letters (J, Z)
- [ ] Other sign languages (BSL, ISL)
- [ ] Cloud deployment

### Long-term
- [ ] Real-time translation
- [ ] Sentence grammar
- [ ] Speech output
- [ ] Accessibility features

## ğŸ¤ Use Cases

### Educational
- Learning ASL
- Teaching deaf students
- Sign language practice
- Interactive tutorials

### Accessibility
- Communication aid
- Emergency situations
- Public services
- Healthcare settings

### Research
- Computer vision projects
- ML education
- Accessibility research
- Human-computer interaction

## ğŸ“š Learning Resources

### Included Documentation
1. **README.md** - Complete reference
2. **QUICKSTART.md** - Get started fast
3. **Code comments** - Inline documentation
4. **utils.py** - Diagnostic tools

### External Resources
- [MediaPipe Documentation](https://mediapipe.dev/)
- [ASL Alphabet Guide](https://www.startasl.com/american-sign-language-alphabet/)
- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)
- [OpenCV Documentation](https://opencv.org/)

## ğŸŒŸ Credits & Acknowledgments

### Technologies Used
- **MediaPipe** - Hand tracking
- **TensorFlow** - Deep learning
- **OpenCV** - Computer vision
- **Streamlit** - Web interface
- **NumPy** - Numerical computing
- **Scikit-learn** - ML utilities

### Community
- ASL community for sign language resources
- Open source contributors
- Computer vision researchers

## ğŸ“ Support & Contributing

### Get Help
1. Check documentation (README.md, QUICKSTART.md)
2. Run diagnostics: `python utils.py`
3. Open GitHub issue
4. Review troubleshooting section

### Contribute
1. Fork repository
2. Create feature branch
3. Make improvements
4. Submit pull request

### Report Bugs
- Use GitHub issues
- Include error messages
- Describe steps to reproduce
- Attach screenshots if relevant

## ğŸ“„ License

This project is licensed under the MIT License - see LICENSE file for details.

---

**Made with â¤ï¸ for accessibility and education**

*Last updated: October 2025*

